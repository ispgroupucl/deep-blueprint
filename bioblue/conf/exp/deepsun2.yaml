# @package _global_
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 30
  gpus: ${gpus}
  auto_scale_batch_size: false
  log_every_n_steps: 2


module:
  # lr: 1.
  # optimizer: 'torch.optim.SGD'
  # loss:
  #   _target_: bioblue.loss.GDiceLoss
  #   apply_nonlin : softmax
  #   # to_onehot_y : true

  # lr: 1.e-1
  # optimizer: 'torch.optim.SGD'
  # loss:
  #   _target_: bioblue.loss.TverskyLossV3
  #   alpha: 0.3
  #   beta: 0.7 
  #   include_background: false
  #   softmax: true
  #   to_onehot_target: true
  # ######## WORKS
  # lr: 1.e-1
  # optimizer: 'torch.optim.SGD'
  # loss:
  #   _target_: bioblue.loss.DiceLoss
  # ######## WORKS
  lr: 1.e-5
  loss:
    _target_: torch.nn.CrossEntropyLoss # <--- WORKS
    # ignore_index: 0
  class_weights: [1,20,20]
  # ########

  # scheduler:
  #   _target_: torch.optim.lr_scheduler.MultiStepLR
  #   milestones: [10, 30, 50]
  #   gamma: 0.1

model:
  input_format: ["image"]
  output_format: ["segmentation"]
  # classes: ["penumbra", "umbra"]
  classes: [ "penumbra", "umbra"]
  _target_: bioblue.model.ConfUnet
  model_cfg:
    _target_: bioblue.model.ModelConfig
  block: double
  architecture:
    first: 32
    enc:
      width: [32, 64, 128, 256, 512, 512, 512]
      repeat: [1,1,1,1,1,1,1]
      # width: [32, 64, 128, 256, 512]
      # repeat: [1,1,1,1,1]
    dec:
      # width: [ 256, 128, 64, 32]
      # repeat: [ 1, 1, 1, 1]
      width: [512, 512, 256, 128, 64, 32]
      repeat: [1, 1, 1, 1, 1, 1]

  # architecture:
  #   first: 32 
  #   enc:
  #     width: [16, 32, 48, 96]
  #     repeat: [2, 3, 3, 4]
  #   dec:
  #     width: [48, 32, 32]
  #     repeat: [2, 2, 1]


use_dtypes: "???"

dataset:
  _target_: bioblue.dataset.BioblueDataModule
  data_dir: "/globalscratch/users/n/s/nsayez/deepsun_bioblue"
  dataset_name: "All"
  # dataset_name: "2013"
  batch_size: 6
  num_workers: 3
  
  train_dataset:
    _target_: bioblue.dataset.DeepsunSegmentationDataset
    partition: "test_GT"
    dtypes: ${use_dtypes}

    transforms:
      - _target_: bioblue.transforms.DeepsunCropNonEmptyMaskIfExists
        height: 512
        width: 512
        

  val_dataset:
    _target_: bioblue.dataset.DeepsunSegmentationDataset
    # \/!\/!\/!\/!\/!\/ When training make sure that this partition refers to folder containing ALL types of segmentation, and the folder with GT is named "test_GT"
    # \/!\/!\/!\/!\/!\/ BUT When comparing with manual GT -> CHANGE NAME OF FOLDERS SUCH THAT the one with GT is named test and the other is named "test_training"
    partition: "test_GT"
    dtypes: ${use_dtypes}
    transforms:
      - _target_: bioblue.transforms.DeepsunCropNonEmptyMaskIfExists
        height: 512
        width: 512
  test_dataset: ${dataset.val_dataset}

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: "./models"
    save_last: true
    monitor: val_meaniou
    mode: max
    filename: "{epoch}-{val_meaniou:.2f}"
  LR_monitor:
    _target_: "pytorch_lightning.callbacks.LearningRateMonitor"
  Save_prediction:
    _target_: 'bioblue.callback.SavePredictionMaskCallback'
    output_dir: 'predictions'
    max_batch_size: ${dataset.batch_size}
