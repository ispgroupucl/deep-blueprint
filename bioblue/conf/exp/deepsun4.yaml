# @package _global_
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 30
  gpus: ${gpus}
  auto_scale_batch_size: false
  log_every_n_steps: 2


module:
  # ######## WORKS
  lr: 1.e-5
  loss:
    _target_: torch.nn.CrossEntropyLoss # <--- WORKS
    # _target_: torch.nn.BCELoss 
    # ignore_index: 0
  class_weights: [1,20]
  # ########
  # loss:
  #   _target_: bioblue.loss.CombineLosses
  #   sublossA: 
  #     _target_: torch.nn.CrossEntropyLoss
  #     # class_weights: [1,20,20]
  #   sublossB: 
  #     _target_: bioblue.loss.TverskyLossV3
  #     alpha: 0.3
  #     beta: 0.7
  #     include_background: false
  #     softmax: true
  #     to_onehot_target: true
  #   ce_ratio: 0.5
  #   include_background: false
  # class_weights: [1,20,20]
  ##############
  # loss:
  #   _target_: bioblue.loss.ComboLoss  
  #   ce_ratio: 0.5
  #   alpha: 0.6
  #   include_background: true
  #   softmax: true
  #   to_onehot_target: true
    
  # ########
  # loss:
  #   _target_: bioblue.loss.LogCosHDiceLoss
  #   softmax: true
  #   to_onehot_target: true
  #   include_background: false

  # scheduler:
  #   _target_: torch.optim.lr_scheduler.MultiStepLR
  #   milestones: [10, 30, 50]
  #   gamma: 0.1

model:
  input_format: ["image"]
  output_format: ["segmentation"]
  # classes: [ "penumbra", "umbra"]
  classes: [ "sunspot"]
  _target_: bioblue.model.ConfUnet
  model_cfg:
    _target_: bioblue.model.ModelConfig
  block: double
  architecture:
    first: 32
    enc:
      width: [32, 64, 128, 256, 512, 512, 512]
      repeat: [1,1,1,1,1,1,1]
    dec:
      width: [512, 512, 256, 128, 64, 32]
      repeat: [1, 1, 1, 1, 1, 1]



use_dtypes: "???"

dataset:
  _target_: bioblue.dataset.BioblueDataModule
  data_dir: "/DATA/datasets/niels/Deepsun/bioblue/"
  dataset_name: "ManualAnnotation"
  batch_size: 6
  num_workers: 3
  
  train_dataset:
    _target_: bioblue.dataset.DeepsunSegmentationDataset
    partition: "test_GT"
    dtypes: ${use_dtypes}

    transforms:
      - _target_: bioblue.transforms.DeepsunCropNonEmptyMaskIfExists
        height: 512
        width: 512
      - _target_: bioblue.transforms.DeepsunMaskMerger
        p_add: 0.5
      - _target_: bioblue.transforms.DeepsunRandomFlip
      - _target_: bioblue.transforms.DeepsunRandomRotate
        

  val_dataset:
    _target_: bioblue.dataset.DeepsunSegmentationDataset
    # \/!\/!\/!\/!\/!\/ When training make sure that this partition refers to folder containing ALL types of segmentation, and the folder with GT is named "test_GT"
    # \/!\/!\/!\/!\/!\/ BUT When comparing with manual GT -> CHANGE NAME OF FOLDERS SUCH THAT the one with GT is named test and the other is named "test_training"
    partition: "test_GT"
    dtypes: ${use_dtypes}
    transforms:
      - _target_: bioblue.transforms.DeepsunCropNonEmptyMaskIfExists
        height: 512
        width: 512
      - _target_: bioblue.transforms.DeepsunMaskMerger
        p_add: 0.5
  test_dataset: ${dataset.val_dataset}

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: "./models"
    save_last: true
    monitor: val_meaniou
    mode: max
    filename: "{epoch}-{val_meaniou:.2f}"
  LR_monitor:
    _target_: "pytorch_lightning.callbacks.LearningRateMonitor"
  Save_prediction:
    _target_: 'bioblue.callback.SavePredictionMaskCallback'
    output_dir: 'predictions'
    max_batch_size: ${dataset.batch_size}
